"""
This module measures performance of quickoffice build, it works on top of c2c
tool with slide modification (ie. it launches only 1 instance of browser)

"""
__author__ = 'pavan.gupta@synerzip.com'

import os
import logging
from subprocess import call
import shutil
import subprocess
from prop_parser import Properties


class QoUtil(object):
    """
    This class is for quickoffice utilities which are generic and can be used
    for other modules also.
    """
    # global LOGGER

    def count_char(self, char, string):
        """
        This method will count the occurrence of a given character in
        argument str
    Args:
        char: Character for which frequency need to be counted
        str: Source string
    Return:
        Frequency of character
        """
        count = 0
        for charecter in string:
            if charecter == char:
                count += 1
        return count

    def get_time_format(self, minute, sec, millisec, split_char=':'):
        """
        Convert given parameters into standard time
        format(i.e. min:sec:millisec)
    Args:
        min: Minutes
        sec: Seconds
        millisec: MilliSeconds
        split_char: separator for time elements
        """
        formated_time = str(minute).rjust(2, '0') + \
                        split_char + str(sec).rjust(2, '0') + \
                        split_char + str(millisec).rjust(3, '0')
        return formated_time

    def open_file(self, file_name, mode):
        """
            a wrapper to open function with exception handling
    Args:
        fileName: Name of file to be opened
        mode: Mode(like r,w,a) in which file will be opened
        """
        try:
            data = open(file_name, mode)
            return data
        except IOError as err:
            data.close()

    def remove_all_files(self, dir_path, rm_dir_tree=False):
        """
        remove all files in given directory path
        Note: this method will not delete nested directories
        """
        if rm_dir_tree == True:
            shutil.rmtree(dir_path)
        else:
            file_list = [flst for flst in os.listdir(dir_path)]
            for flst in file_list:
                file_path = os.path.join(dir_path, flst)
                if os.path.isfile(file_path):
                    os.remove(file_path)

    def write_dict_to_csv(self, dictionary, dict_first_key,
                          dict_last_key, file_path):
        """
        Write a dictionary in csv file
    Args:
        dict: dictionary which need to be written csv file
        file_path: target csv file which would be generated by this method
        """

        with open(file_path, 'w') as fl_obj:
        # Below code is to keep <b> File Name </b> key on the top of CSV File
            try:
                fl_obj.write(dict_first_key)
                fl_obj.write(str(dictionary.pop(dict_first_key)))
                fl_obj.write('\n')

                lst = list(dictionary.keys())
                # key is used for sorting list by file extension
                lst.sort(key=lambda lst: (os.path.splitext(lst)[1],
                                          os.path.splitext(lst)[0]))
                #print lst

                for k in lst:
                    if k == dict_last_key:
                        continue
                    fl_obj.write(k)
                    fl_obj.write(dictionary[k])
                    fl_obj.write('\n')

                fl_obj.write(dict_last_key)
                fl_obj.write(str(dictionary[dict_last_key]))
            except KeyError as key:
                print "Key {0} does not exist in dictionary".format(str(key))

    def execute_py(self, no_of_iteration, csv_generator,
                   source_csv_file_path, csv_generator_args,
                   output_csv_files_dir):
        """
        This will execute c2c python code

        """
        for i in range(no_of_iteration):
            call(["/usr/bin/python", csv_generator, csv_generator_args,
                  'BLESSED'])
            #os.system(cmd)
            tcsv = output_csv_files_dir + '/run0' + str(i + 1) + '.csv'
            # Calling mv command of unix to move csv file
            call('mv -f {0} {1}'.format(source_csv_file_path, tcsv), shell=True)

    def print_dict(self, dict_data, separator=':'):
        """
            print given dictionary and put separator between key and value
            while printing
        """
        for key, value in dict_data.items():
            print str(key) + separator + '   ' + str(value)

    # def remove_keys(self, lst, dictionary):
    #     """
    #         Remove all keys based on given list
    #     """
    #     for item in lst:
    #         dictionary.pop(item, None)

    def is_dict_empty(self, dictionary):
        """
        checks whether given dictionary is empty or not
        """
        isempty = not (dictionary and True) or False
        return isempty

    def list_from_dict_value(self, dict_data, value):
        """
            returns list of keys by value.
        """
        lst = []
        for key, value in dict_data.items():
            if value == value:
                lst.append(key)
        return lst


class Performance(object):
    """
     This class contains function which be helpful in measuring the performance
     of all quickoffice applications(i.e. word/sheet/point)
    """
    pdata = {}
    flist_data = {}                 # To store File name data of all csv files
    unmatched_data = {}             # To store unmatched data of all csv files

    pdata_first_key = 'File Name'
    pdata_last_key = 'Total Load Time Of Iterations:'

    total_load_time_of_iterations = ''
    csv_file_count = 0
    qu_obj = QoUtil()

    def __init__(self, test_files_dir, target_csv_file):
        """
            initializing Performance class variables
        """
        self.test_files_dir = test_files_dir
        self.target_performance_csv_file = target_csv_file
        #print "Inside Performance Constructor"

    def remove_non_csv_files(self, test_files_dir_path):
        """
        this method will delete files other then csv
    Args:
    test_files_dir_path: test files source directory path
        """
        file_list = os.listdir(test_files_dir_path)
        for item in file_list:
            if not item.endswith('.csv'):
                try:
                    os.remove(test_files_dir_path + '/' + item)
                except OSError, ex_obj:
                    print "Error in {0} : {1}".format(str(ex_obj.filename), str(ex_obj.strerror))

    def convert_time_format(self, time_part):
        """
        this method convert time format from <b> min-sec.millisec </b> to <b>
        min:sec:millisec </b>
    Args:
        time_part:
        """
        try:
            minute_time_part = str(time_part.split('-')[0]).replace('\n', '')\
                .rjust(2, '0')
            if not '-' in time_part:
                seconds_time_part = '00'
            else:
                seconds_time_part = (str(time_part.split('-')[1])
                                     .split('.')[0]).replace('\n', '')\
                    .rjust(2, '0')

            if not '.' in time_part:
                millisec_time_part = '000'
            else:
                millisec_time_part = (((str(time_part.split('-')[1])
                                    .split('.')[1])[:3]).replace('\n', ''))\
                    .ljust(3, '0')
        except BaseException as ex:
           print 'Incorrect input format for time_part: ' + str(ex.message)
           return -1

        return '{0}:{1}:{2}'.format(minute_time_part,
                                    seconds_time_part,
                                    millisec_time_part)

    def add_time_list(self, time_list, list_split_char=' ',
                      time_split_char=':'):
        """
          generates total of time list
    Args:
        time_list: list of elements(each representing time in
        min:sec:Millisecond) format list_split_char: separator for list items
        time_split_char: separator for each part(which is representing time
        like min,sec, millisec) of item
    Return:
        Total of all list items
        """
        total_min = 0
        total_second = 0
        total_millisec = 0

        #print time_list

        scrutinized_list = (str(time_list).replace(',', "").strip())\
            .split(list_split_char)
        for line in scrutinized_list:
            try:
                minute, sec, millisec = line.replace(',', '').strip()\
                    .split(time_split_char)
                total_min += int(minute)
                total_second += int(sec)
                total_millisec += int(millisec)
            except ValueError as ve_obj:
               print '{0}: {1} need more than 1 value to unpack'.format(str(ve_obj), line)

            #print min +":"+ sec +":"+ millisec

        total_second += (total_millisec / 1000)
        total_min += (total_second / 60)

        total_time = self.qu_obj.get_time_format(total_min, str(total_second %
                                                             60),
                                        str(total_millisec % 1000))

        return total_time

    def calc_avg_time(self, total_time, no_of_files, time_split_char=':'):
        """
        calculate average time for no of files
    Args:
    total_time: single time instance(like min:sec:millisec) which is total of
    time taken by one file in all iterations
    no_of_files: dividing factor for average calculation of time

        """
        total_millisec = 0
        minute, sec, millisec = str(total_time).strip().split(time_split_char)
        total_millisec = int(millisec) + (int(sec) * 1000) \
                         + (int(minute) * 60 * 1000)
        # Mean time in milliseconds

        try:
            mean_time = total_millisec / no_of_files
            hour = mean_time / (60 * 60 * 1000)
            mean_time = mean_time % (60 * 60 * 1000)
            minute = mean_time / (60 * 1000)
            mean_time = mean_time % (60 * 1000)
            sec = mean_time / (1000)
            mean_time = mean_time % 1000
            millisec = mean_time

            #print min + ':' + sec + ':' + millisec
            return str(hour).rjust(2, '0') + ':' + str(minute).rjust(2, '0') \
                   + ':' + str(sec).rjust(2, '0') + '.' \
                   + str(millisec).rjust(3, '0')

            #return self.qu.get_time_format(min, sec, millisec)
        except ZeroDivisionError as ex_obj:
            print 'No of files is zero: %s' + str(ex_obj)
            return

    def add_csv_file(self, file_name, mode):
        """
        analyze one csv file and add it to dictionary
    Args:
        file: csv file to be analyzed
        mode: file opening mode like r,w,a.
        """

        file_obj = None
        total_load_time = 0
        total_time_in_second = 0.0
        try:
            file_obj = self.qu_obj.open_file(file_name, mode)
            try:
                for line in file_obj:
                    time_part = line.rpartition(':')[2]
                    first_part = line.split(':', 2)[0]

                    #print self.convert_time_format(timePart)
                    if first_part not in self.pdata:
                        self.pdata[first_part] = ', '\
                                    + str(self.convert_time_format(time_part))
                    else:
                        self.pdata[first_part] += ', ' \
                                    + str(self.convert_time_format(time_part))

                    total_load_time += int(time_part.split('-')[0])
                    total_time_in_second += float(time_part.split('-')[1])
            except ValueError as ex:
                print "Incompatible number format: " + str(ex)
            file_obj.close()
        except IOError as ex:
            print "File can not be opened" + str(ex)
            file_obj.close()

        total_load_time += int(total_time_in_second / 60)
        total_load_time = str(total_load_time) + '-' \
                        + (str(total_time_in_second % 60)[:6])
        self.total_load_time_of_iterations += ', ' \
                                + self.convert_time_format(str(total_load_time))

    def get_last_column_to_string(self, element_separator=' '):
        """
        returns last column of dictionary as string, separated by space
    Args:
    element_separator: Element separator used in concatenated string
    return: concatenated string(element separator) constructed by last column

        """
        ttstr = ''
        # calculating vertical total time
        for k in self.pdata:
            # below condition will skipp additional keys(File Name and
            # Total Load Time Of Iterations)
            if k == self.pdata_first_key or k == self.pdata_last_key:
                continue
            ttstr = ttstr + (str(self.pdata[k]))[str(
                self.pdata[k]).rfind(element_separator):]
        return ttstr

    def fill_dict(self):
        """
            filling dictionary with required data, which is required to put in
            CSV files
        """

        # filling file names in dictionary and additional value for total and
        # average time
        if os.path.isdir(self.test_files_dir):
            flist = os.listdir(self.test_files_dir)
            self.pdata[self.pdata_first_key] = str(flist).replace("'", '')

            #self.pdata[self.pdata_first_key] = ', '
            # + (self.pdata[self.pdata_first_key])[1:-1] + \
            #                                   ', Total Time, Average Time '
            self.pdata[self.pdata_first_key] = ', ' \
                + (self.pdata[self.pdata_first_key])[1:-1] + ', Average Time '

            for item in flist:
                self.add_csv_file(self.test_files_dir + '/' + item, 'r')
                self.csv_file_count += 1

            # adding total time to dictionary
            total_time = ''
            for k in self.pdata:
                #skipping additional key(File Name and Total Load Time Of
                # Iterations)
                if k == self.pdata_first_key or k == self.pdata_last_key:
                    continue
                total_time += self.add_time_list(self.pdata[k]) + ' '
                #self.pdata[k] += ', ' + self.add_time_list(self.pdata[k])

            # adding Average Time to dictionary
            # below line is commented because removing Total Time column from
            # dictionary
            #last_column = self.get_last_column_to_string().strip().split(' ')
            bkp_last_column = total_time.strip().split(' ')
            lst_count = 0
            for k in self.pdata:
                if k == self.pdata_first_key:
                    continue
                # below line is commented because removing Total Time column
                # from dictionary
                #self.pdata[k] += ', '
                # + self.calc_avg_time(last_column[lst_count],
                #                                           self.csv_file_count)
                self.pdata[k] += ', ' + self.calc_avg_time(
                    bkp_last_column[lst_count], self.csv_file_count)
                lst_count += 1

            # adding Total time of iterations to dictionary
            # Please do not move below line on top of above for loop that will
            # scrud the calculations of <b> Total Time </b> and <b> Average Time
            # </b> Column
            self.pdata[self.pdata_last_key] = self.total_load_time_of_iterations
            grand_total = self.add_time_list(self.pdata[self.pdata_last_key])

            #self.pdata[self.pdata_last_key] += ', ' + self.add_time_list(
            #    self.pdata[self.pdata_last_key])
            #grand_total=(str(self.pdata[self.pdata_last_key]))[str(
            #    self.pdata[self.pdata_last_key]).rfind(' '):]
            self.pdata[self.pdata_last_key] += ', ' \
            + self.calc_avg_time(grand_total, self.csv_file_count)
        else:
            pass

    def validate_pdata(self, expected_columns):
        """
        Validate csv data on the basis of column present.
        Args:
    expected_columns: Would depend on No of iteration (iteration + 1)
        """
        #d={'1':'1,2,3,4,5', '2':'f,d,g,h', '3':'d,fg,d,fg,dfg,d'}
        for k in self.pdata:
            if not expected_columns == self.qu_obj.count_char(',', self.pdata[
                k]):
                print k + ' is missing in one of the csv'
                #self.pdata.pop(k)

   

    def print_dict(self):
        """
        print dictionary elements on console
        """
        print (self.pdata_first_key + '\t' * 3).ljust(63) \
              + str(self.pdata[self.pdata_first_key])
        i = 1
        for k in self.pdata:
            if k == self.pdata_first_key or k == self.pdata_last_key:
                continue
            print str(i) + '  ' + str(k).ljust(64) + '\t\t' \
                  + str(self.pdata[k]).replace('\n', ',\t\t\t\t')
            i += 1
            #self.pdata[self.pdata_last_key] = ', '
            # + (str(self.total_load_time_of_iterations)
        #                                               [1:-1]).replace("'", '')


        print self.pdata_last_key + '\t' * 10 + \
              self.pdata[self.pdata_last_key] + '\t'
        print '*' * 140

    def get_filenames_data(self):
        """
        Will fetch filename part form all csv file of test_files_dir and put
        them in flist_data

        """
        lst = os.listdir(self.test_files_dir)
        lst.sort()
        # print lst

        for item in lst:
            # print f
            # print '***' * 40
            flist = subprocess.check_output(['cut', '-d', ':', '-f', '1',
                                             self.test_files_dir + '/' + item])
            self.flist_data[item] = flist.split('\n')
            if '' in self.flist_data[item]:
                (self.flist_data[item]).remove('') # Remove additional empty
                # line
        return self.flist_data

    def unmatched_data_from_files(self, intersection_list):
        """
        This method will return dictionary of unmatched data present in all
        csv files
intersection_list: list of common files of all input csv files

        """
        print '*' * 40 + '     Intersection(Matched) List     ' + '*' * 40
        for item in intersection_list:
            print item

        print '*' * 40 + '     Missing(Unmatched) Data     ' + '*' * 40
        for k in self.flist_data:

            if set(self.flist_data[k]) == intersection_list:
                continue
            self.unmatched_data[k] = set(self.flist_data[k]) - intersection_list

        self.qu_obj.print_dict(self.unmatched_data)
        print '*' * 105

        return self.unmatched_data

    def intersection(self):
        """
        returns intersection set of flist_data values
        """
        self.get_filenames_data()

        # Below set(flist_data[k] will contain set of lists
        inter_lists = [set(self.flist_data[k]) for k in self.flist_data.keys()]
        intersection_list = set.intersection(*inter_lists)

        # for l in intersection_list:
        #     print l

        # filling unmatched data
        return intersection_list
        # print inter_list
        # print len(inter_list)
        # print len(flist_data['run01.csv'])

    def update_csv_files(self):
        """
        update all csv files of test_files_dir as per unmatched data
        """
        # unmatched data is having files which have missing data(not all
        # files of self.test_files_dir)
        if not self.qu_obj.is_dict_empty(self.unmatched_data):
            for key_file in self.unmatched_data.keys():
                fin = open(self.test_files_dir + '/' + key_file)
                fout = open(self.test_files_dir + '/u_' + key_file, 'w')
                for line in fin:
                    for item in self.unmatched_data[key_file]:
                        if (line.split(':')[0]) == item:
                            # print f + ':  ' + line
                            break
                    else:
                        fout.write(line)
                fin.close()
                fout.close()
                # Removing old files with updated files
                try:
                    os.remove(self.test_files_dir + '/' + key_file)
                except OSError, ex_obj:
                ## if failed, report it back to the user ##
                    print ("Error: {0} - {1}".format(str(ex_obj.filename),
                                                     str(ex_obj.strerror)))
                subprocess.call(
                    ['mv', self.test_files_dir + '/u_' + key_file,
                     self.test_files_dir
                                                       + '/' + key_file])
        else:
            print "All CSV File inside '{0}' have matching data only"\
                .format(self.test_files_dir)

if __name__ == '__main__':

    p = Properties("/Users/pavang/Documents/practice/python2/performance.property")
    p.load()
    # p.print_prop()

    #Performance
    OUTPUT_CSV_FILES_DIR = p.get('OUTPUT_CSV_FILES_DIR')
    TARGET_PERFORMANCE_CSV_FILE = p.get('TARGET_PERFORMANCE_CSV_FILE')

    #QoUtil
#     LOG_FILE_PATH = p.get('LOG_FILE_PATH')
    NO_OF_ITERATION = int(p.get('NO_OF_ITERATION'))
    CSV_GENERATOR = p.get('CSV_GENERATOR')
    CSV_GENERATOR_ARGS = p.get('CSV_GENERATOR_ARGS')
    SOURCE_CSV_FILE_PATH = p.get('SOURCE_CSV_FILE_PATH')

    PERF = Performance(OUTPUT_CSV_FILES_DIR,
                       TARGET_PERFORMANCE_CSV_FILE)
    QU_OBJ = QoUtil()

    print '=' * 100
    print "Deleting Non CSV files"
    PERF.remove_non_csv_files(OUTPUT_CSV_FILES_DIR)

    print "Filling data in Dictionary"
    PERF.fill_dict()

    print "Validating dictionary data"
    PERF.validate_pdata(NO_OF_ITERATION + 1)

    PERF.print_dict()
    print "Performance data written in: " + TARGET_PERFORMANCE_CSV_FILE
    QU_OBJ.write_dict_to_csv(PERF.pdata, PERF.pdata_first_key,
                             PERF.pdata_last_key,
                             PERF.target_performance_csv_file)

    print "**********************     Done     **************************"
